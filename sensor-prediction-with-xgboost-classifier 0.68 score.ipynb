{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:12:33.309189Z","iopub.execute_input":"2025-06-26T08:12:33.309605Z","iopub.status.idle":"2025-06-26T08:12:35.326525Z","shell.execute_reply.started":"2025-06-26T08:12:33.309573Z","shell.execute_reply":"2025-06-26T08:12:35.325554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#score - 0.68","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:12:38.159263Z","iopub.execute_input":"2025-06-26T08:12:38.160722Z","iopub.status.idle":"2025-06-26T08:12:38.166305Z","shell.execute_reply.started":"2025-06-26T08:12:38.160658Z","shell.execute_reply":"2025-06-26T08:12:38.165265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1_demo=pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv\")\ndf1_demo.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:12:39.697592Z","iopub.execute_input":"2025-06-26T08:12:39.698751Z","iopub.status.idle":"2025-06-26T08:13:19.763273Z","shell.execute_reply.started":"2025-06-26T08:12:39.698699Z","shell.execute_reply":"2025-06-26T08:13:19.761650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import skew, kurtosis, iqr, entropy\nfrom scipy.signal import welch\nfrom scipy.fft import fft, fftfreq\nimport pywt\n\ndef zero_crossing_rate(signal):\n    return np.mean(np.diff(np.sign(signal)) != 0)\n\ndef mean_crossing_rate(signal):\n    return np.mean(np.diff(np.sign(signal - np.mean(signal))) != 0)\n\ndef signal_energy(signal):\n    return np.sum(np.square(signal))\n\ndef get_fft_features(signal, fs=50):\n    signal = signal - np.mean(signal)\n    N = len(signal)\n    if N <= 1:\n        return np.nan, np.nan, np.nan, np.nan\n    yf = np.abs(fft(signal))[:N // 2]\n    freqs = fftfreq(N, d=1 / fs)[:N // 2]\n    if yf.sum() == 0:\n        return 0, 0, 0, 0\n    dominant_freq = freqs[np.argmax(yf)]\n    spectral_centroid = np.sum(freqs * yf) / np.sum(yf)\n    power = yf ** 2\n    band_power = np.sum(power)\n    p = power / np.sum(power)\n    spectral_entropy = entropy(p)\n    return dominant_freq, spectral_centroid, spectral_entropy, band_power\n\ndef get_wavelet_features(signal, wavelet='db4'):\n    if len(signal) < 8:\n        return [np.nan] * 4\n    coeffs = pywt.wavedec(signal, wavelet, level=2)\n    features = []\n    for c in coeffs[1:]:  # skip approximation\n        features.append(np.mean(np.square(c)))  # energy\n    return features\n\ndef extract_imu_features(segment):\n    features = {}\n    acc_cols = ['acc_x', 'acc_y', 'acc_z']\n    rot_cols = ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n\n    for col in acc_cols + rot_cols:\n        signal = segment[col].dropna().values\n        if signal.size == 0:\n            for suffix in ['mean', 'std', 'min', 'max', 'range', 'median', 'iqr', 'skew', 'kurt',\n                           'energy', 'zcr', 'mcr', 'fft_dom', 'fft_centroid', 'fft_entropy', 'fft_bandpow',\n                           'wav_energy_1', 'wav_energy_2']:\n                features[f'{col}_{suffix}'] = np.nan\n            continue\n\n        features[f'{col}_mean'] = np.mean(signal)\n        features[f'{col}_std'] = np.std(signal)\n        features[f'{col}_min'] = np.min(signal)\n        features[f'{col}_max'] = np.max(signal)\n        features[f'{col}_range'] = np.ptp(signal)\n        features[f'{col}_median'] = np.median(signal)\n        features[f'{col}_iqr'] = iqr(signal)\n        features[f'{col}_skew'] = skew(signal)\n        features[f'{col}_kurt'] = kurtosis(signal)\n        features[f'{col}_energy'] = signal_energy(signal)\n        features[f'{col}_zcr'] = zero_crossing_rate(signal)\n        features[f'{col}_mcr'] = mean_crossing_rate(signal)\n\n        # FFT-based features\n        dom_freq, centroid, spec_ent, bandpow = get_fft_features(signal)\n        features[f'{col}_fft_dom'] = dom_freq\n        features[f'{col}_fft_centroid'] = centroid\n        features[f'{col}_fft_entropy'] = spec_ent\n        features[f'{col}_fft_bandpow'] = bandpow\n\n        # Wavelet\n        wav_feats = get_wavelet_features(signal)\n        features[f'{col}_wav_energy_1'] = wav_feats[0]\n        features[f'{col}_wav_energy_2'] = wav_feats[1]\n\n    # Jerk (first derivative of acc)\n    acc_data = segment[acc_cols].dropna()\n    if not acc_data.empty:\n        jerk = np.diff(acc_data.values, axis=0)\n        jerk_mag = np.linalg.norm(jerk, axis=1)\n        features['jerk_mean'] = np.mean(jerk_mag)\n        features['jerk_std'] = np.std(jerk_mag)\n    else:\n        features['jerk_mean'] = np.nan\n        features['jerk_std'] = np.nan\n\n    # Magnitude of acc and rot\n    for prefix, cols in zip(['acc', 'rot'], [acc_cols, rot_cols[:3]]):\n        vec = segment[cols].dropna()\n        if not vec.empty:\n            mag = np.linalg.norm(vec.values, axis=1)\n            features[f'{prefix}_mag_mean'] = np.mean(mag)\n            features[f'{prefix}_mag_std'] = np.std(mag)\n        else:\n            features[f'{prefix}_mag_mean'] = np.nan\n            features[f'{prefix}_mag_std'] = np.nan\n\n    return features\n\n\nall_imu_features = []\n\nfor seq_id, segment in df1_demo.groupby('sequence_id'):\n    feats = extract_imu_features(segment)\n    feats['sequence_id'] = seq_id\n    all_imu_features.append(feats)\n\ndf_imu_features = pd.DataFrame(all_imu_features)\ndf_imu_features.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:24:15.279936Z","iopub.execute_input":"2025-06-26T08:24:15.280355Z","iopub.status.idle":"2025-06-26T08:27:20.188666Z","shell.execute_reply.started":"2025-06-26T08:24:15.280324Z","shell.execute_reply":"2025-06-26T08:27:20.187803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import skew, kurtosis\n\ndef extract_thermopile_features(segment):\n    thm_cols = [f'thm_{i}' for i in range(1, 6)]\n    features = {}\n\n    thm_data = segment[thm_cols].copy()\n\n    # --- Per-sensor features ---\n    for col in thm_cols:\n        signal = thm_data[col].dropna().values\n\n        if signal.size == 0:\n            features[f'{col}_mean'] = np.nan\n            features[f'{col}_std'] = np.nan\n            features[f'{col}_range'] = np.nan\n            features[f'{col}_skew'] = np.nan\n            features[f'{col}_kurtosis'] = np.nan\n            features[f'{col}_slope'] = np.nan\n            features[f'{col}_rolling_mean_5'] = np.nan\n            features[f'{col}_rolling_mean_10'] = np.nan\n        else:\n            features[f'{col}_mean'] = np.mean(signal)\n            features[f'{col}_std'] = np.std(signal)\n            features[f'{col}_range'] = np.ptp(signal)\n            features[f'{col}_skew'] = skew(signal)\n            features[f'{col}_kurtosis'] = kurtosis(signal)\n\n            # Temporal slope = (last - first) / n\n            features[f'{col}_slope'] = (signal[-1] - signal[0]) / max(1, len(signal))\n\n            # Rolling window means (pad with same value to preserve shape)\n            rolling_5 = pd.Series(signal).rolling(window=5, min_periods=1).mean()\n            rolling_10 = pd.Series(signal).rolling(window=10, min_periods=1).mean()\n            features[f'{col}_rolling_mean_5'] = rolling_5.mean()\n            features[f'{col}_rolling_mean_10'] = rolling_10.mean()\n\n    # --- Pairwise differences (asymmetry detection) ---\n    for i in range(5):\n        for j in range(i + 1, 5):\n            diff = thm_data[thm_cols[i]] - thm_data[thm_cols[j]]\n            diff_clean = diff.dropna().values\n            if diff_clean.size > 0:\n                features[f'diff_{thm_cols[i]}_{thm_cols[j]}_mean'] = np.mean(diff_clean)\n                features[f'diff_{thm_cols[i]}_{thm_cols[j]}_std'] = np.std(diff_clean)\n            else:\n                features[f'diff_{thm_cols[i]}_{thm_cols[j]}_mean'] = np.nan\n                features[f'diff_{thm_cols[i]}_{thm_cols[j]}_std'] = np.nan\n\n    # --- Global features ---\n    thm_means = thm_data.mean(axis=0).values\n    sensor_positions = np.array([0, 1, 2, 3, 4])  # assume uniform sensor layout\n\n    if not np.isnan(thm_means).all():\n        weighted_sum = np.sum(sensor_positions * thm_means)\n        features['thm_center_of_mass'] = weighted_sum / np.sum(thm_means)\n        features['thm_global_mean'] = np.nanmean(thm_means)\n        features['thm_global_std'] = np.nanstd(thm_means)\n        features['thm_global_range'] = np.nanmax(thm_means) - np.nanmin(thm_means)\n    else:\n        features['thm_center_of_mass'] = np.nan\n        features['thm_global_mean'] = np.nan\n        features['thm_global_std'] = np.nan\n        features['thm_global_range'] = np.nan\n\n    return features\n\n\nall_features = []\nfor seq_id, segment in df1_demo.groupby('sequence_id'):\n    feats = extract_thermopile_features(segment)\n    feats['sequence_id'] = seq_id\n    all_features.append(feats)\n\ndf_therm = pd.DataFrame(all_features)\ndf_therm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:16:06.876639Z","iopub.execute_input":"2025-06-26T08:16:06.877037Z","iopub.status.idle":"2025-06-26T08:18:03.279183Z","shell.execute_reply.started":"2025-06-26T08:16:06.877010Z","shell.execute_reply":"2025-06-26T08:18:03.277464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import entropy\n\ndef extract_tof_features_grouped(segment):\n    features = {}\n    sensor_ids = range(1, 6)\n    pixel_ids = range(64)\n\n    for sensor in sensor_ids:\n        pixel_cols = [f\"tof_{sensor}_v{i}\" for i in pixel_ids if f\"tof_{sensor}_v{i}\" in segment.columns]\n\n        if not pixel_cols:\n            continue\n\n        pixel_data = segment[pixel_cols].replace(-1, np.nan).values.astype(float)\n\n        # Per-sensor stats\n        features[f'tof_{sensor}_mean'] = np.nanmean(pixel_data)\n        features[f'tof_{sensor}_std'] = np.nanstd(pixel_data)\n        features[f'tof_{sensor}_min'] = np.nanmin(pixel_data)\n        features[f'tof_{sensor}_max'] = np.nanmax(pixel_data)\n        features[f'tof_{sensor}_missing_rate'] = np.isnan(pixel_data).sum() / pixel_data.size\n        features[f'tof_{sensor}_nonzero_ratio'] = np.count_nonzero(~np.isnan(pixel_data)) / pixel_data.size\n\n        # Spatial entropy over all frames and pixels\n        flat_vals = pixel_data[~np.isnan(pixel_data)].flatten()\n        if flat_vals.size > 0:\n            hist, _ = np.histogram(flat_vals, bins=10, density=True)\n            hist = hist[hist > 0]\n            features[f'tof_{sensor}_spatial_entropy'] = entropy(hist)\n        else:\n            features[f'tof_{sensor}_spatial_entropy'] = np.nan\n\n        # Center of mass (average across time steps)\n        try:\n            com_x_list, com_y_list = [], []\n            for row in pixel_data:\n                if np.isnan(row).all():\n                    continue\n                grid = row.reshape(8, 8)\n                xx, yy = np.meshgrid(np.arange(8), np.arange(8))\n                grid_masked = np.nan_to_num(grid, nan=0)\n                total = np.sum(grid_masked)\n                if total > 0:\n                    com_x = np.sum(xx * grid_masked) / total\n                    com_y = np.sum(yy * grid_masked) / total\n                    com_x_list.append(com_x)\n                    com_y_list.append(com_y)\n\n            if com_x_list:\n                features[f'tof_{sensor}_com_x'] = np.mean(com_x_list)\n                features[f'tof_{sensor}_com_y'] = np.mean(com_y_list)\n            else:\n                features[f'tof_{sensor}_com_x'] = np.nan\n                features[f'tof_{sensor}_com_y'] = np.nan\n        except:\n            features[f'tof_{sensor}_com_x'] = np.nan\n            features[f'tof_{sensor}_com_y'] = np.nan\n\n    return features\n\n\n# Assuming df1_demo contains your full dataset\nall_tof_features = []\n\nfor seq_id, segment in df1_demo.groupby('sequence_id'):\n    feats = extract_tof_features_grouped(segment)\n    feats['sequence_id'] = seq_id\n    all_tof_features.append(feats)\n\ndf_tof = pd.DataFrame(all_tof_features)\ndf_tof.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:18:03.280685Z","iopub.execute_input":"2025-06-26T08:18:03.281026Z","iopub.status.idle":"2025-06-26T08:22:45.517996Z","shell.execute_reply.started":"2025-06-26T08:18:03.281001Z","shell.execute_reply":"2025-06-26T08:22:45.516559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_tof.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:22:45.519155Z","iopub.execute_input":"2025-06-26T08:22:45.519461Z","iopub.status.idle":"2025-06-26T08:22:45.543208Z","shell.execute_reply.started":"2025-06-26T08:22:45.519433Z","shell.execute_reply":"2025-06-26T08:22:45.541918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_tof_temporal_features(df):\n    temporal_feats = []\n    tof_cols = [col for col in df.columns if col.startswith(\"tof_\") and '_v' in col]\n\n    for seq_id, segment in df.groupby(\"sequence_id\"):\n        segment_sorted = segment.sort_values(\"sequence_counter\")[tof_cols].replace(-1, np.nan).fillna(0).values\n        diffs = np.diff(segment_sorted, axis=0)  # Shape: (n-1, 320)\n\n        feats = {\n            'sequence_id': seq_id,\n            'tof_diff_mean': np.mean(diffs),\n            'tof_diff_std': np.std(diffs),\n            'tof_diff_max': np.max(diffs),\n            'tof_diff_min': np.min(diffs),\n        }\n        temporal_feats.append(feats)\n\n    return pd.DataFrame(temporal_feats)\n\n\ndf_tof_temp=extract_tof_temporal_features(df1_demo)\ndf_tof_temp.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:22:45.545138Z","iopub.execute_input":"2025-06-26T08:22:45.545630Z","iopub.status.idle":"2025-06-26T08:23:00.936118Z","shell.execute_reply.started":"2025-06-26T08:22:45.545596Z","shell.execute_reply":"2025-06-26T08:23:00.935152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dup_gest=df1_demo[['sequence_id','gesture']].drop_duplicates()\nall_data=df_imu_features.merge(df_therm,on=['sequence_id']).merge(df_tof,on=['sequence_id']).merge(df_tof_temp,on=['sequence_id']).merge(dup_gest,on=['sequence_id'])\nall_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:27:47.830272Z","iopub.execute_input":"2025-06-26T08:27:47.830695Z","iopub.status.idle":"2025-06-26T08:27:48.030733Z","shell.execute_reply.started":"2025-06-26T08:27:47.830667Z","shell.execute_reply":"2025-06-26T08:27:48.029699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df2_demo=pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv\")\n# df2_demo.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:34.903788Z","iopub.execute_input":"2025-06-19T15:15:34.904037Z","iopub.status.idle":"2025-06-19T15:15:34.922212Z","shell.execute_reply.started":"2025-06-19T15:15:34.904019Z","shell.execute_reply":"2025-06-19T15:15:34.921168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df3_demo=df1_demo.merge(df2_demo,on=['subject'],how='left')\n# df3_demo.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:34.923521Z","iopub.execute_input":"2025-06-19T15:15:34.923959Z","iopub.status.idle":"2025-06-19T15:15:35.648991Z","shell.execute_reply.started":"2025-06-19T15:15:34.923920Z","shell.execute_reply":"2025-06-19T15:15:35.648057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_df=pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\")\n# test_demo=pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\")\n# test_df=test_df.merge(test_demo,on=['subject'],how='left')\n# test_df_1=test_df.drop(['row_id', 'sequence_id','sequence_counter','subject'],axis=1)\n# test_df_1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:35.650029Z","iopub.execute_input":"2025-06-19T15:15:35.650294Z","iopub.status.idle":"2025-06-19T15:15:35.689958Z","shell.execute_reply.started":"2025-06-19T15:15:35.650272Z","shell.execute_reply":"2025-06-19T15:15:35.688973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df3_demo=df3_demo.fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:35.691119Z","iopub.execute_input":"2025-06-19T15:15:35.691464Z","iopub.status.idle":"2025-06-19T15:15:37.154813Z","shell.execute_reply.started":"2025-06-19T15:15:35.691433Z","shell.execute_reply":"2025-06-19T15:15:37.153719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_df_1=test_df_1.fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:37.155733Z","iopub.execute_input":"2025-06-19T15:15:37.155987Z","iopub.status.idle":"2025-06-19T15:15:37.162383Z","shell.execute_reply.started":"2025-06-19T15:15:37.155968Z","shell.execute_reply":"2025-06-19T15:15:37.161069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X1=df3_demo.drop(['row_id', 'sequence_id','sequence_counter','subject'],axis=1)\n# X=df3_demo.drop(['row_id', 'sequence_id','sequence_counter','subject','gesture'],axis=1)\n# y=df3_demo['gesture']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:37.163521Z","iopub.execute_input":"2025-06-19T15:15:37.163835Z","iopub.status.idle":"2025-06-19T15:15:38.291504Z","shell.execute_reply.started":"2025-06-19T15:15:37.163812Z","shell.execute_reply":"2025-06-19T15:15:38.290176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:28:04.105554Z","iopub.execute_input":"2025-06-26T08:28:04.105955Z","iopub.status.idle":"2025-06-26T08:28:26.677276Z","shell.execute_reply.started":"2025-06-26T08:28:04.105925Z","shell.execute_reply":"2025-06-26T08:28:26.676120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\n# def encode_categorical_columns(df):\n#     df_encoded = X1.copy()\n#     encoders = {}\n    \n#     for col in df_encoded.columns:\n#         if df_encoded[col].dtype == 'object' or str(df_encoded[col].dtype) == 'category':\n#             #le = LabelEncoder()\n#             df_encoded[col] = le.fit_transform(df_encoded[col])\n#             encoders[col] = le  # Store encoder for inverse transform later\n#             print(f\"Encoded '{col}' → {list(le.classes_)}\")\n    \n#     return df_encoded, encoders\n\n# df_encoded, encoders = encode_categorical_columns(X1)\n\n# print(\"\\nEncoded DataFrame:\\n\", df_encoded)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:28:26.679032Z","iopub.execute_input":"2025-06-26T08:28:26.680169Z","iopub.status.idle":"2025-06-26T08:28:26.684871Z","shell.execute_reply.started":"2025-06-26T08:28:26.680137Z","shell.execute_reply":"2025-06-26T08:28:26.684002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dep_var=X1['gesture']\n#dep_var_1=le.fit(dep_var)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:45.143863Z","iopub.execute_input":"2025-06-19T15:15:45.144539Z","iopub.status.idle":"2025-06-19T15:15:45.148955Z","shell.execute_reply.started":"2025-06-19T15:15:45.144507Z","shell.execute_reply":"2025-06-19T15:15:45.147822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# def encode_categorical_columns(df):\n#     test_encoded = test_df_1.copy()\n#     test_encoders = {}\n    \n#     for col in test_encoded.columns:\n#         if test_encoded[col].dtype == 'object' or str(test_encoded[col].dtype) == 'category':\n#             #le = LabelEncoder()\n#             test_encoded[col] = le.fit_transform(test_encoded[col])\n#             test_encoders[col] = le  # Store encoder for inverse transform later\n#             print(f\"Encoded '{col}' → {list(le.classes_)}\")\n    \n#     return test_encoded, test_encoders\n\n# test_encoded, test_encoders = encode_categorical_columns(test_df_1)\n\n# print(\"\\nEncoded DataFrame:\\n\", test_encoded)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:28:29.087844Z","iopub.execute_input":"2025-06-26T08:28:29.088225Z","iopub.status.idle":"2025-06-26T08:28:29.093056Z","shell.execute_reply.started":"2025-06-26T08:28:29.088199Z","shell.execute_reply":"2025-06-26T08:28:29.092023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.preprocessing import MinMaxScaler\n# # Initialize the scaler\n# scaler = MinMaxScaler()\n\n# # Fit and transform the data\n# scaled_data = scaler.fit_transform(X2)\n\n# # Convert back to DataFrame (optional)\n# scaled_df = pd.DataFrame(scaled_data, columns=X2.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:45.687956Z","iopub.execute_input":"2025-06-19T15:15:45.688428Z","iopub.status.idle":"2025-06-19T15:15:48.692697Z","shell.execute_reply.started":"2025-06-19T15:15:45.688395Z","shell.execute_reply":"2025-06-19T15:15:48.691391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.datasets import fetch_openml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:15:48.693755Z","iopub.execute_input":"2025-06-19T15:15:48.694059Z","iopub.status.idle":"2025-06-19T15:15:48.772817Z","shell.execute_reply.started":"2025-06-19T15:15:48.694037Z","shell.execute_reply":"2025-06-19T15:15:48.771846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Split\n# # Step 2: Split into Train (60%), Validation (20%), Test (20%)\n# y=df_encoded['gesture']\n# X3=df3_demo[test_df_1.columns]\n\n# # Split numeric and categorical\n# num_cols = X3.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# cat_cols = X3.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# # Pipelines for each type\n# numeric_transformer = Pipeline([\n#     ('scaler', StandardScaler()),\n#     ('pca', PCA(n_components=50))  # Choose n_components based on variance or elbow\n# ])\n\n# categorical_transformer = Pipeline([\n#     ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n#     ('svd', TruncatedSVD(n_components=1))  # Works on high-dimensional one-hot data\n# ])\n\n# # Combine transformations\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numeric_transformer, num_cols),\n#         ('cat', categorical_transformer, cat_cols)\n#     ]\n# )\n\n# # Apply transformation\n# X_reduced = preprocessor.fit_transform(X3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:22:47.945614Z","iopub.execute_input":"2025-06-19T15:22:47.946015Z","iopub.status.idle":"2025-06-19T15:23:16.493050Z","shell.execute_reply.started":"2025-06-19T15:22:47.945991Z","shell.execute_reply":"2025-06-19T15:23:16.485814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X=all_data.drop(['gesture','sequence_id'],axis=1)\ny_b4=all_data['gesture']\ny=pd.DataFrame(le.fit_transform(y_b4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:34:07.394620Z","iopub.execute_input":"2025-06-26T08:34:07.395428Z","iopub.status.idle":"2025-06-26T08:34:07.415382Z","shell.execute_reply.started":"2025-06-26T08:34:07.395374Z","shell.execute_reply":"2025-06-26T08:34:07.414086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, test_size=0.25, random_state=42, stratify=y_train_full)\n# (0.25 of 0.8 = 0.2 --> 60/20/20 split)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:34:08.803877Z","iopub.execute_input":"2025-06-26T08:34:08.804288Z","iopub.status.idle":"2025-06-26T08:34:08.898148Z","shell.execute_reply.started":"2025-06-26T08:34:08.804259Z","shell.execute_reply":"2025-06-26T08:34:08.897223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.datasets import load_iris  # you can use your dataset instead\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:34:11.129169Z","iopub.execute_input":"2025-06-26T08:34:11.129603Z","iopub.status.idle":"2025-06-26T08:34:11.135177Z","shell.execute_reply.started":"2025-06-26T08:34:11.129572Z","shell.execute_reply":"2025-06-26T08:34:11.134222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Define XGBoost multi-class classifier\nmodel = XGBClassifier(\n    objective='multi:softprob',\n    num_class=len(y.drop_duplicates()),\n    eval_metric='mlogloss',\n    use_label_encoder=True,\n    random_state=42,\n    enable_categorical='True'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:34:12.835370Z","iopub.execute_input":"2025-06-26T08:34:12.835778Z","iopub.status.idle":"2025-06-26T08:34:12.843301Z","shell.execute_reply.started":"2025-06-26T08:34:12.835749Z","shell.execute_reply":"2025-06-26T08:34:12.842256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model on selected features\nmodel.fit(X_train, y_train, \n          eval_set=[(X_val, y_val)],\n          early_stopping_rounds=10,\n          verbose=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:34:14.430595Z","iopub.execute_input":"2025-06-26T08:34:14.430964Z","iopub.status.idle":"2025-06-26T08:34:55.553002Z","shell.execute_reply.started":"2025-06-26T08:34:14.430940Z","shell.execute_reply":"2025-06-26T08:34:55.552019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:34:55.554293Z","iopub.execute_input":"2025-06-26T08:34:55.554578Z","iopub.status.idle":"2025-06-26T08:34:55.558865Z","shell.execute_reply.started":"2025-06-26T08:34:55.554557Z","shell.execute_reply":"2025-06-26T08:34:55.557919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Evaluate on validation set\nval_preds = model.predict(X_val)\nval_accuracy = accuracy_score(y_val, val_preds)\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"\\nValidation Classification Report:\\n\", classification_report(y_val, val_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:34:55.559796Z","iopub.execute_input":"2025-06-26T08:34:55.560158Z","iopub.status.idle":"2025-06-26T08:34:55.644249Z","shell.execute_reply.started":"2025-06-26T08:34:55.560134Z","shell.execute_reply":"2025-06-26T08:34:55.643220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Final test evaluation\ntest_preds = model.predict(X_test)\ntest_accuracy = accuracy_score(y_test, test_preds)\nprint(\"Test Accuracy:\", test_accuracy)\nprint(\"\\nTest Classification Report:\\n\", classification_report(y_test, test_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:34:55.645948Z","iopub.execute_input":"2025-06-26T08:34:55.646230Z","iopub.status.idle":"2025-06-26T08:34:55.713734Z","shell.execute_reply.started":"2025-06-26T08:34:55.646209Z","shell.execute_reply":"2025-06-26T08:34:55.712948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_full_1, X_test_1, y_train_full_1, y_test_1 = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\n\nX_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(\n    X_train_full_1, y_train_full_1, test_size=0.25, random_state=42, stratify=y_train_full)\n# (0.25 of 0.8 = 0.2 --> 60/20/20 split)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:36:17.654198Z","iopub.execute_input":"2025-06-26T08:36:17.655136Z","iopub.status.idle":"2025-06-26T08:36:17.744749Z","shell.execute_reply.started":"2025-06-26T08:36:17.655097Z","shell.execute_reply":"2025-06-26T08:36:17.743649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_preds=pd.DataFrame(model.predict(X_test))\ntest_preds.columns=['gesture_pred']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:36:19.908830Z","iopub.execute_input":"2025-06-26T08:36:19.909194Z","iopub.status.idle":"2025-06-26T08:36:19.968851Z","shell.execute_reply.started":"2025-06-26T08:36:19.909170Z","shell.execute_reply":"2025-06-26T08:36:19.968051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# binary f1 score and macro f1 for validation data set\ndup_test=df1_demo[['sequence_type','gesture']].drop_duplicates()\ndup_test.columns=['seq_type_pred','gesture_pred']\n# Rename one or more columns\n\ndup_org=df1_demo[['sequence_type','gesture']].drop_duplicates()\n\n\ntest_preds=pd.DataFrame(model.predict(X_test))\ntest_preds.columns=['gesture_pred']\ntest_preds=pd.DataFrame(le.inverse_transform(test_preds['gesture_pred']))\ntest_preds.columns=['gesture_pred']\nX_test_1['gesture_pred']=test_preds\n#X_test_1['gesture_pred']=le.inverse_transform(X_test_1['gesture_pred'])\ntest_preds=test_preds.merge(dup_test,on=['gesture_pred'],how='left')\ntest_preds.head()\n\ny_test_1=pd.DataFrame(le.inverse_transform(y_test))\ny_test_1.columns=['gesture']\ny_test_1=y_test_1.merge(dup_org,on=['gesture'],how='left')\ny_test_1.head()\n\ny_test_1['sequence_type']=np.where(y_test_1['sequence_type']=='Target',1,0)\ntest_preds['seq_type_pred']=np.where(test_preds['seq_type_pred']=='Target',1,0)\n\n\ntest_preds['gesture_pred']=np.where(test_preds['seq_type_pred']==1,test_preds['gesture_pred'],\"Non-Target\")\ny_test_1['gesture']=np.where(y_test_1['sequence_type']==1,y_test_1['gesture'],\"Non-Target\")\n\n\nfrom sklearn.metrics import f1_score\n# Compute binary F1 score\nf1 = f1_score(y_test_1['sequence_type'], test_preds['seq_type_pred'])\nprint(f\"Binary F1 Score (Target vs Non-Target): {f1:.4f}\")\n\n# Compute macro F1 score\nf1_macro = f1_score(y_test_1['gesture'], test_preds['gesture_pred'], average='macro')\nprint(f\"Macro F1 Score (with non-target collapsed): {f1_macro:.4f}\")\n\nfinal_score=(f1+f1_macro)/2\nprint(f\"final overall score:{final_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:36:21.365192Z","iopub.execute_input":"2025-06-26T08:36:21.365700Z","iopub.status.idle":"2025-06-26T08:36:21.638434Z","shell.execute_reply.started":"2025-06-26T08:36:21.365669Z","shell.execute_reply":"2025-06-26T08:36:21.636985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# binary f1 score and macro f1 for validation data set\ndup_test=df1_demo[['sequence_type','gesture']].drop_duplicates()\ndup_test.columns=['seq_type_pred','gesture_pred']\n# Rename one or more columns\n\ndup_org=df1_demo[['sequence_type','gesture']].drop_duplicates()\n\n\nval_preds=pd.DataFrame(model.predict(X_val))\nval_preds.columns=['gesture_pred']\nval_preds['gesture_pred']=le.inverse_transform(val_preds['gesture_pred'])\nval_preds=val_preds.merge(dup_test,on=['gesture_pred'],how='left')\nval_preds.head()\n\ny_val_1=pd.DataFrame(le.inverse_transform(y_val))\ny_val_1.columns=['gesture']\ny_val_1=y_val_1.merge(dup_org,on=['gesture'],how='left')\ny_val_1.head()\n\ny_val_1['sequence_type']=np.where(y_val_1['sequence_type']=='Target',1,0)\nval_preds['seq_type_pred']=np.where(val_preds['seq_type_pred']=='Target',1,0)\n\n\nval_preds['gesture_pred']=np.where(val_preds['seq_type_pred']==1,val_preds['gesture_pred'],\"Non-Target\")\ny_val_1['gesture']=np.where(y_val_1['sequence_type']==1,y_val_1['gesture'],\"Non-Target\")\n\n\nfrom sklearn.metrics import f1_score\n# Compute binary F1 score\nf1 = f1_score(y_val_1['sequence_type'], val_preds['seq_type_pred'])\nprint(f\"Binary F1 Score (Target vs Non-Target): {f1:.4f}\")\n\n# Compute macro F1 score\nf1_macro = f1_score(y_val_1['gesture'], val_preds['gesture_pred'], average='macro')\nprint(f\"Macro F1 Score (with non-target collapsed): {f1_macro:.4f}\")\n\nfinal_score=(f1+f1_macro)/2\nprint(f\"final overall score:{final_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:36:29.261854Z","iopub.execute_input":"2025-06-26T08:36:29.262253Z","iopub.status.idle":"2025-06-26T08:36:29.524638Z","shell.execute_reply.started":"2025-06-26T08:36:29.262224Z","shell.execute_reply":"2025-06-26T08:36:29.523742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df=pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\")\n\nfor seq_id, segment in test_df.groupby('sequence_id'):\n    feats = extract_imu_features(segment)\n    feats['sequence_id'] = seq_id\n    all_imu_features.append(feats)\n\ntest_imu_features = pd.DataFrame(all_imu_features)\ntest_imu_features.head()\n\nall_features = []\nfor seq_id, segment in test_df.groupby('sequence_id'):\n    feats = extract_thermopile_features(segment)\n    feats['sequence_id'] = seq_id\n    all_features.append(feats)\n\ntest_therm = pd.DataFrame(all_features)\ntest_therm\n\n\n\n# Assuming df1_demo contains your full dataset\nall_tof_features = []\n\nfor seq_id, segment in test_df.groupby('sequence_id'):\n    feats = extract_tof_features_grouped(segment)\n    feats['sequence_id'] = seq_id\n    all_tof_features.append(feats)\n\ntest_tof = pd.DataFrame(all_tof_features)\ntest_tof.head()\n\n\ntest_tof_temp=extract_tof_temporal_features(test_df)\ntest_tof_temp.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:45:38.821605Z","iopub.execute_input":"2025-06-26T08:45:38.822129Z","iopub.status.idle":"2025-06-26T08:45:39.752863Z","shell.execute_reply.started":"2025-06-26T08:45:38.822090Z","shell.execute_reply":"2025-06-26T08:45:39.751984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data=test_imu_features.merge(test_therm,on=['sequence_id']).merge(test_tof,on=['sequence_id']).merge(test_tof_temp,on=['sequence_id'])\ntest_data.head()\ntest_data_v1=test_data.drop(['sequence_id'],axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:47:13.644227Z","iopub.execute_input":"2025-06-26T08:47:13.644618Z","iopub.status.idle":"2025-06-26T08:47:13.662770Z","shell.execute_reply.started":"2025-06-26T08:47:13.644589Z","shell.execute_reply":"2025-06-26T08:47:13.661701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntest_data['gesture_pred']=pd.DataFrame(model.predict(test_data_v1))\ntest_data['gesture_pred']=le.inverse_transform(test_data['gesture_pred'])\ntest_df=test_df.merge(test_data[['sequence_id','gesture_pred']],on=['sequence_id'])\ntest_df_final=test_df[['sequence_id','gesture_pred']]\ntest_df_final.columns=['sequence_id','gesture']\n#dup_org=df1_demo[['sequence_type','gesture']].drop_duplicates()\n#test_df_final=test_df_final.merge(dup_org,on=['gesture'],how='left')\n#test_df_final['gesture']=np.where(test_df_final['sequence_type']==\"Target\",test_df_final['gesture'],\"Non-Target\")\n#test_df_final=test_df_final[['sequence_id','gesture']]\ntest_df_final.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:47:51.906344Z","iopub.execute_input":"2025-06-26T08:47:51.907298Z","iopub.status.idle":"2025-06-26T08:47:51.941114Z","shell.execute_reply.started":"2025-06-26T08:47:51.907263Z","shell.execute_reply":"2025-06-26T08:47:51.940190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df_final.to_parquet(\"submission.parquet\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:48:07.224382Z","iopub.execute_input":"2025-06-26T08:48:07.225317Z","iopub.status.idle":"2025-06-26T08:48:07.303951Z","shell.execute_reply.started":"2025-06-26T08:48:07.225284Z","shell.execute_reply":"2025-06-26T08:48:07.302985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport polars as pl\n\nimport kaggle_evaluation.cmi_inference_server\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:54:16.674226Z","iopub.execute_input":"2025-06-26T08:54:16.674629Z","iopub.status.idle":"2025-06-26T08:54:17.768769Z","shell.execute_reply.started":"2025-06-26T08:54:16.674599Z","shell.execute_reply":"2025-06-26T08:54:17.767791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    test_df=  sequence.to_pandas()\n    test_demo= demographics.to_pandas()\n    # test_df=test_df.merge(test_demo,on=['subject'],how='left')\n    # test_df_1=test_df.drop(['row_id', 'sequence_id','sequence_counter','subject'],axis=1)\n    # test_df_1=test_df_1.fillna(0)\n    # test_X_reduced = preprocessor.fit_transform(test_df_1)\n\n\n    for seq_id, segment in test_df.groupby('sequence_id'):\n        feats = extract_imu_features(segment)\n        feats['sequence_id'] = seq_id\n        all_imu_features.append(feats)\n\n    test_imu_features = pd.DataFrame(all_imu_features)\n    test_imu_features.head()\n\n\n    all_features = []\n    for seq_id, segment in test_df.groupby('sequence_id'):\n        feats = extract_thermopile_features(segment)\n        feats['sequence_id'] = seq_id\n        all_features.append(feats)\n\n    test_therm = pd.DataFrame(all_features)\n    test_therm\n\n\n    # Assuming df1_demo contains your full dataset\n    all_tof_features = []\n\n    for seq_id, segment in test_df.groupby('sequence_id'):\n        feats = extract_tof_features_grouped(segment)\n        feats['sequence_id'] = seq_id\n        all_tof_features.append(feats)\n\n    test_tof = pd.DataFrame(all_tof_features)\n    test_tof.head()\n\n\n    test_tof_temp=extract_tof_temporal_features(test_df)\n    test_tof_temp.head()\n\n    test_data=test_imu_features.merge(test_therm,on=['sequence_id']).merge(test_tof,on=['sequence_id']).merge(test_tof_temp,on=['sequence_id'])\n    test_data_v1=test_data.drop(['sequence_id'],axis=1)\n\n    \n    \n    test_data['gesture_pred']=pd.DataFrame(model.predict(test_data_v1))\n    test_data['gesture_pred']=le.inverse_transform(test_data['gesture_pred'])\n    test_df=test_df.merge(test_data[['sequence_id','gesture_pred']],on=['sequence_id'])\n    test_df_final=test_df[['sequence_id','gesture_pred']]\n    test_df_final.columns=['sequence_id','gesture']\n    \n    \n    \n    return str(test_df_final['gesture'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:54:18.534374Z","iopub.execute_input":"2025-06-26T08:54:18.535079Z","iopub.status.idle":"2025-06-26T08:54:18.545026Z","shell.execute_reply.started":"2025-06-26T08:54:18.535046Z","shell.execute_reply":"2025-06-26T08:54:18.544003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:54:21.492469Z","iopub.execute_input":"2025-06-26T08:54:21.492855Z","iopub.status.idle":"2025-06-26T08:54:23.999057Z","shell.execute_reply.started":"2025-06-26T08:54:21.492824Z","shell.execute_reply":"2025-06-26T08:54:23.998031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#sample synethetic data bro\n\n\n# # --- 1. Generate Synthetic Sensor Data (Replace with your actual data loading) ---\n# # For demonstration purposes, let's create some synthetic time-series sensor data.\n# # Imagine 1000 samples, each with 50 time steps and 3 sensor features (e.g., X, Y, Z acceleration)\n\n# num_samples = 1000\n# time_steps = 50\n# num_features = 3\n\n# # Simulate different patterns for two classes\n# # Class 0: More sinusoidal\n# t = np.linspace(0, 10 * np.pi, time_steps)\n# data_class_0 = np.array([np.sin(t + np.random.rand() * 2 * np.pi) * 0.5 + np.random.rand(time_steps) * 0.1 for _ in range(num_samples // 2)])\n# data_class_0 = np.stack([data_class_0, data_class_0 * 0.8, data_class_0 * 1.2], axis=-1) # Add 3 features\n\n# # Class 1: More spikey/noisy\n# data_class_1 = np.array([np.random.rand(time_steps) * 0.8 + np.sin(t / 2 + np.random.rand() * 2 * np.pi) * 0.2 for _ in range(num_samples // 2)])\n# data_class_1 = np.stack([data_class_1 * 1.5, data_class_1, data_class_1 * 0.7], axis=-1) # Add 3 features\n\n# X = np.vstack((data_class_0, data_class_1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:20:02.059883Z","iopub.status.idle":"2025-06-19T15:20:02.060186Z","shell.execute_reply.started":"2025-06-19T15:20:02.060052Z","shell.execute_reply":"2025-06-19T15:20:02.060064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}